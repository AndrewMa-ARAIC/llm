# -*- coding: utf-8 -*-
"""llama-cpp-python test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vyT4D4LQuNaNE5HbRZlrKDMw3nB9Sj3z
"""

model = "mistral-7b-openorca.Q2_K.gguf"

# from huggingface_hub import hf_hub_download
# hf_hub_download(repo_id="TheBloke/Mistral-7B-OpenOrca-GGUF",
#                 filename=model,
#                 local_dir="./",
#                 )

from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# Callbacks support token-wise streaming
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

# Make sure the model path is correct for your system!
llm = LlamaCpp(
    model_path=f"./{model}",
    temperature=0.75,
    max_tokens=2000,
    top_p=1,
    callback_manager=callback_manager,
    verbose=True,  # Verbose is required to pass to the callback manager
)

llm(
    """<|im_start|>system
You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!
<|im_end|>
<|im_start|>user
What is the capital of Taiwan?
<|im_end|>
<|im_start|>assistant
"""
)
